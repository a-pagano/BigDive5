{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Reduce Explained \n",
    "\n",
    "Is a programming model invented by Google to perform distributed computation on huge amounts of data, it's heavily inspired by the functional programming **map** and **reduce** functions.\n",
    "\n",
    "The process is divided in two steps:\n",
    " - A mapping step: the input is divided in subparts until it reaches its minimum processable particle which gets processed and the corresponding result gets returned.\n",
    " - A reduction step: each value processed by the mapping step goes back to the reduction step that joins the values together to a single result.\n",
    "\n",
    "<img src=\"files/MapReduce.jpeg\" width=\"640\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Practice Example\n",
    "\n",
    "<img src=\"files/MapReduce_example.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Example Problem without MapReduce\n",
    "\n",
    "\n",
    "Our example problem is to compute the sum of the doubles of various numbers.\n",
    "\n",
    "So if we have 1, 2, 3 as the input the result would be 2+4+6 -> 12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999000\n"
     ]
    }
   ],
   "source": [
    "numbers = range(1000)\n",
    "\n",
    "def doubled_sum(values):\n",
    "    total = 0\n",
    "    for n in numbers:\n",
    "        total += n*2\n",
    "    return total\n",
    "\n",
    "print doubled_sum(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Problem using MapReduce\n",
    "\n",
    "\n",
    "As Python implements the functional programming paradigm too, it provides the functions required to implement the map-reduce paradigm builtin.\n",
    "\n",
    "The foundation tools to implement map reduce are:\n",
    "\n",
    " - *\"mapper\"* which is in charge of mapping each input value to a corresponding output value\n",
    " - *\"reducer\"* which is in charge of merging multiple mapper outputs into a single output. \n",
    " \n",
    "Both phases can be called multiple times (the output of a reducer can become the input of another reducer and a mapper can call other mappers).\n",
    "\n",
    "Many MapReduce implementations also have additional phases like *\"combination\"* and *\"aggregation\"* which are executed after the mapper or the reducer to further cleanup their output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999000\n"
     ]
    }
   ],
   "source": [
    "numbers = range(1000)\n",
    "\n",
    "def mapper(value):\n",
    "    return value*2\n",
    "\n",
    "def reducer(*values):\n",
    "    return sum(values)\n",
    "#first_step = map(mapper, numbers)\n",
    "#print first_step\n",
    "result = reduce(reducer, map(mapper, numbers))\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous map-reduce in pure python implementation lacks of course the core feature of MapReduce: **working parallely**.\n",
    "\n",
    "It's easy to understand that as each *mapper* and *reducer* works only on a subset of the data (its own input) it can work independently from the status of the other mappers and reducers. So the computation can proceed parallely.\n",
    "\n",
    "## Parallel Map Reduce in Pure Python\n",
    "\n",
    "\n",
    "It's really easy to simulate a parallel map reduce in python by using the multiprocessing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "class ParallelMapReduce(object):\n",
    "    def __init__(self, map_func, reduce_func, num_workers=None):\n",
    "        self.num_workers = num_workers\n",
    "        self.map_func = map_func\n",
    "        self.reduce_func = reduce_func\n",
    "        self.pool = multiprocessing.Pool(num_workers)\n",
    "    \n",
    "    def partition(self, n, iterable):\n",
    "        i = iter(iterable)\n",
    "        piece = list(islice(i, n))\n",
    "        while piece:\n",
    "            yield piece\n",
    "            piece = list(islice(i, n))\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        values = self.pool.map(self.map_func, inputs)\n",
    "        \n",
    "        print '>>> MAPPED VALUES (%s values): %s, ...' % (len(values), str(values[:10]))\n",
    "\n",
    "        values = self.pool.map(self.reduce_func, \n",
    "                               self.partition(len(values)//self.num_workers, values))\n",
    "        print '>>> REDUCED VALUES', values\n",
    "\n",
    "        return self.reduce_func(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous mapreduce implementation takes a Mapper and a Reducer and splits them across *num_workers* until it gets back the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> MAPPED VALUES (1000 values): [0, 2, 4, 6, 8, 10, 12, 14, 16, 18], ...\n",
      ">>> REDUCED VALUES [9900, 29900, 49900, 69900, 89900, 109900, 129900, 149900, 169900, 189900]\n",
      "999000\n"
     ]
    }
   ],
   "source": [
    "numbers = range(1000)\n",
    "\n",
    "def mapper(value):\n",
    "    return value*2\n",
    "\n",
    "def reducer(values):\n",
    "    return sum(values)\n",
    "\n",
    "mapreduce = ParallelMapReduce(mapper, reducer, 10)\n",
    "print mapreduce(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Map Reduce\n",
    "\n",
    "Our parallel map reduce works parallely by using the multiple cores our computer provides, but it is unable to distribute the work on multiple computers. As map reduce is actually meant to work on milions of items it is vital to be able to distribute the work across multiple servers.\n",
    "\n",
    "This is the reason why various MapReduce frameworks and toolkits were created, on Python environment there are some wildly used solutions:\n",
    "\n",
    "### Disco ###\n",
    "\n",
    "Born at Nokia Research centers it is one of the solutions with the best tradeof in complexity and features. It relies on an ERlang core involved in dispatching jobs to the various workers and Python is considered the standard language used to implement workers (while they can be written in any language respecting the Disco protocol). \n",
    "\n",
    "It also provides its own Distributed File System (DDFS) and Distributed Database (DiscoDB) which can be used to store data shared by the various workers.\n",
    "\n",
    "### OctoPy ###\n",
    "\n",
    "It's a pure Python solution that is aimed at being really quick to setup, doesn't provide a shared storage for data.\n",
    "Can be useful for really small problems where the cost of configuring a cluster can overcome the solution benefit.\n",
    "\n",
    "### Hadoop ###\n",
    "\n",
    "It's the standard de facto in MapReduce frameworks, heavily implemented in Java it's the most complex solution to setup and maintain. Through the Hadoop Streaming feature it is possible to run mappers and reducers in any programming language that can be called by a shell script. Hadoop Streaming it is used be various Python libraries to run python implemented mappers and reducers. \n",
    "\n",
    "Like Disco it's a full fledged solution that provides also a Distributed File System where data can be stored for processing **HDFS**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon EMR - Elastic MapReduce\n",
    "\n",
    "\n",
    "EMR, **Elastic Map Reduce** is an Amazon provided Hadoop cluster in the cloud.\n",
    "\n",
    "As setting up a complex MapReduce toolkit like Hadoop is a complex and long procedure, it is common to rely on services that provide the computation cluster on demand. During the BigDive course we are going to test our solutions locally, but we will see also how to run them on EMR. You will notice that while EMR has a huge kickstart cost, it quickly becames more convienient when data increases\n",
    "\n",
    "It relies on **Elastic Compute Cloud** (EC2) to power up a set of computational instances that will became nodes of the Hadoop cluster and on **Simple Storage Service** (S3) to provide a distribuited file system to which each computation node can access data.\n",
    "\n",
    "<img src=\"files/ec2.png\" width=\"640\"/>\n",
    "\n",
    "### Kickstart Cost ###\n",
    "\n",
    "The issue with using EMR is that you have a big kickstart cost caused by:\n",
    "\n",
    " - EC2 instances need to be created and fully booted to start processing data\n",
    " - Your data has to be uploaded to an S3 bucket for instances to be able to read it and process it\n",
    " \n",
    "For this reason, as the kickstart cost of EMR takes minutes, we will test most examples locally using the multiprocessing module to emulate Hadoop nodes. But we will see that our code is perfectly able to run on EMR without changes, so that you can start using EMR as soon as your data gets big enough to justify the kickstart cost.\n",
    "\n",
    "\n",
    "# Hadoop Streaming\n",
    "\n",
    "The feature that permits to run map reduce processes written in any language on hadoop is called Hadoop Streaming,\n",
    "Hadoop Streaming is a tool that takes as a parameter any runnable software and runs it as a mapper or as a reducer.\n",
    "\n",
    "This makes possible to pass as mappers and reducers Python scripts, which will be started in the Hadoop Cluster.\n",
    "\n",
    "For input and output a line based protocol (data separated by \"\\n\") is used:\n",
    "\n",
    " - Each \"line\" of the input sent to the mapper has to be considered one value to map\n",
    " - Each \"line\" of the output from the mapper is condidered an emitted (key, value) pair where key and value are tab (\"\\t\") separated\n",
    " \n",
    "So the typical I/O in hadoop streaming looks like:\n",
    "\n",
    "`key1\\tvalue1\\nkey2\\tvalue2\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
