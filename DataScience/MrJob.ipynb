{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MrJob\n",
    "\n",
    "MrJob is a MapReduce framework which is able to run mapper and reducers on:\n",
    "\n",
    " - Locally using multiprocessing\n",
    " - Hadoop Cluster\n",
    " - Amazon EMR\n",
    " \n",
    "It's a powerfull abstraction over the underlying cluster which takes care for you to save the data on the file system used by the underlying cluster and starting the processing nodes. Your python code won't change in case you run it locally, on hadoop or on EMR.\n",
    "\n",
    "MrJob can be installed using \n",
    "    \n",
    "    $ pip install mrjob\n",
    "    \n",
    "in your python environment.\n",
    "\n",
    "## Protocols\n",
    "\n",
    "As Hadoop Streaming I/O protocol is line based you usually need to take care of properly splitting your data in lines and to properly escape any newline or tab character inside the data.\n",
    "\n",
    "This is something MrJob will do for you through the use of **Protocols**, whenever you declare a MrJob Process you can tell it to encode/decode data using the ``RawValueProtocol`` which uses the standard Hadoop protocol or the ``JSONProtocol`` which encoded the values to JSON to permit representing something more complex than plain text.\n",
    "\n",
    "## MrJob Steps\n",
    "\n",
    "\n",
    "MrJob provides far more steps than the classical Map/Reduce steps.\n",
    "\n",
    "The more classic step is the **combiner** which gets executed after the mapper and before the reducer. This can combine multiple values emitted by the same mapper into a single output. It is usually similar to a **reducer** but it runs only on a specific mapper instead of getting data from any mapper.\n",
    "\n",
    "The full list of all the available steps is:\n",
    "\n",
    "### Main Steps ###\n",
    "- mapper()\n",
    "- combiner()\n",
    "- reducer()\n",
    "\n",
    "### Initialization ###\n",
    "- mapper_init()\n",
    "- combiner_init()\n",
    "- reducer_init()\n",
    "\n",
    "### Finalization ###\n",
    "- mapper_final()\n",
    "- combiner_final()\n",
    "- reducer_final()\n",
    "\n",
    "### Filtering ###\n",
    "- mapper_pre_filter()\n",
    "- combiner_pre_filter()\n",
    "- reducer_pre_filter()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing MrJob Processes\n",
    "\n",
    "\n",
    "MrJob processes are created by declaring a class that inherits from ``mrjob.job.MRJob`` and declares methods named after the MapReduce steps. Each method is required to *emit* a key, value pair through the use of ``yield`` expression.\n",
    "\n",
    "This makes so that each MapReduce steps doesn't have to remember the values it has to emit, it is a generator that sends values to the next step as soon as they are emitted and goes on emitting new values.\n",
    "\n",
    "Due to the way *Hadoop Streaming* works each MrJob process must be a standalone python module which can be started by command line. This is achieved in Python using the\n",
    "\n",
    "```\n",
    "if __name__ == '__main__':\n",
    "   do_something()\n",
    "```\n",
    "\n",
    "which makes so that the module is an executable (``__name__`` is __main__ only when the .py file is started from command line). Whenever the script is started as a standalone executable and not imported by another python software ``do_something()`` will trigger.\n",
    "\n",
    "For MrJob to work instead of do something we will have ``MrJobProcessName.run()`` which actually starts the MrJob process.\n",
    "\n",
    "The reason for this is that your **.py** file has to be copied to the Hadoop Cluster to be executed and then it must be possible to run it as a standalone software that gets started by the Hadoop Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        MRWordFreqCount.run()\n",
    "    except TypeError:\n",
    "        print 'MrJob cannot work inside iPython Notebook as it is not saved as a standalone .py file'\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting MrJob\n",
    "\n",
    "Supponing you saved the previous scripts as **wordcount.py** you can start it using:\n",
    "\n",
    "```\n",
    "$ python 00_wordcount.py lorem.txt\n",
    "```\n",
    "\n",
    "Where lorem.txt is the input of the data (in this case plain text):\n",
    "\n",
    "```\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Quisque molestie lacus a iaculis tempus. Nam lorem nulla, viverra non pulvinar ut, fermentum et tortor. Cras vitae libero sed purus venenatis posuere. Proin commodo risus augue, vitae suscipit lectus accumsan sit amet. Praesent eu erat sem. Pellentesque interdum porta libero, et ultrices nunc eleifend sit amet. In in mauris nec elit ullamcorper ultrices at ac ante. Suspendisse potenti. Aenean eu nisl in ante adipiscing imperdiet. Ut pulvinar lectus quis feugiat adipiscing.\n",
    "Nunc vulputate mauris congue diam ultrices aliquet. Nulla pharetra laoreet est quis vestibulum. Quisque feugiat pharetra sagittis. Phasellus nulla massa, sodales a suscipit blandit, facilisis eu augue. Cras mi massa, ullamcorper nec tristique at, convallis quis eros. Mauris non fermentum lacus, vitae tristique tellus. In volutpat metus augue, nec laoreet ante hendrerit vitae. Vivamus id lacus nec orci tristique vulputate.\n",
    "```\n",
    "\n",
    "When you run the mrjob process you will get something like:\n",
    "\n",
    "```\n",
    "creating tmp directory /var/folders/js/ykgc_8hj10n1fmh3pzdkw2w40000gn/T/wordcount.amol.20140611.163251.274075\n",
    "writing to /var/folders/js/ykgc_8hj10n1fmh3pzdkw2w40000gn/T/wordcount.amol.20140611.163251.274075/step-0-mapper_part-00000\n",
    "Counters from step 1:\n",
    "  (no counters found)\n",
    "writing to /var/folders/js/ykgc_8hj10n1fmh3pzdkw2w40000gn/T/wordcount.amol.20140611.163251.274075/step-0-mapper-sorted\n",
    "> sort /var/folders/js/ykgc_8hj10n1fmh3pzdkw2w40000gn/T/wordcount.amol.20140611.163251.274075/step-0-mapper_part-00000\n",
    "writing to /var/folders/js/ykgc_8hj10n1fmh3pzdkw2w40000gn/T/wordcount.amol.20140611.163251.274075/step-0-reducer_part-00000\n",
    "Counters from step 1:\n",
    "  (no counters found)\n",
    "Moving /var/folders/js/ykgc_8hj10n1fmh3pzdkw2w40000gn/T/wordcount.amol.20140611.163251.274075/step-0-reducer_part-00000 -> /var/folders/js/ykgc_8hj10n1fmh3pzdkw2w40000gn/T/wordcount.amol.20140611.163251.274075/output/part-00000\n",
    "```\n",
    "\n",
    "And final output will be\n",
    "\n",
    "```\n",
    "Streaming final output from /var/folders/js/ykgc_8hj10n1fmh3pzdkw2w40000gn/T/wordcount.amol.20140611.163251.274075/output\n",
    "\"a\"\t2\n",
    "\"ac\"\t1\n",
    "\"accumsan\"\t1\n",
    "\"adipiscing\"\t3\n",
    "\"aenean\"\t1\n",
    "\"aliquet.\"\t1\n",
    "```\n",
    "\n",
    "Note that MrJob is taking care of copying the input and outputs of the various steps to temporary directories (in this case on my own computer as I'm not starting it on an hadoop cluster), and then returns the output.\n",
    "\n",
    "You can note that the output is encoded as by Hadoop Streaming protocol with the word and the count separated by tab as those are the key and value emitted by our reducer.\n",
    "\n",
    "## Hint\n",
    "\n",
    "To save automatically your result inside a file you can simple use command bash command to redirect your output:\n",
    "\n",
    "```\n",
    "$ python 00_wordcount.py lorem.txt > result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can be emitted?\n",
    "\n",
    "We currently counted the words, which is the same example used to explain MapReduce, what if we want to do something else where the data we emit doesn't come from the input itself?\n",
    "\n",
    "MapReduce doesn't make any assumption on the data you emit, nor the key nor the value have to be correlated to the input in any way. Actually each mapper can emit any number of data too.\n",
    "\n",
    "So if we want to get some statistics on the text, like words, characters and phrases we can easily achieve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class MRTextInfo(MRJob):\n",
    "    def mapper(self, _, line):\n",
    "        for phrase in line.split('.'):\n",
    "            yield 'phrases', 1\n",
    "            for word in phrase.split():\n",
    "                yield 'words', 1\n",
    "                yield 'characters', len(word)\n",
    "\n",
    "    def reducer(self, key, counts):\n",
    "        yield key, sum(counts)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        MRTextInfo.run()\n",
    "    except TypeError:\n",
    "        print 'MrJob cannot work inside iPython Notebook as it is not saved as a standalone .py file'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final output will be something like:\n",
    "\n",
    "Creating temp directory /var/folders/_x/g5brlyv963vclshf_kffdm440000gn/T/01_text_info.alexcomu.20160610.150420.364592\n",
    "\n",
    "Running step 1 of 1...\n",
    "\n",
    "Streaming final output from /var/folders/_x/g5brlyv963vclshf_kffdm440000gn/T/01_text_info.alexcomu.20160610.150420.364592/output...\n",
    "\n",
    "    \"characters\"\t1258\n",
    "    \"phrases\"\t21\n",
    "    \"words\"\t144\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiStep Jobs\n",
    "\n",
    "There are cases when it is convenient to run multiple MapReduce steps on a single input to actually provide the expected output.\n",
    "\n",
    "If you want to have multiple steps instead of the plain mapper, reducer steps you can specify them in MrJob using ``steps()`` method which will be called by MrJob to know the actual MapReduce Steps to run in place of the standard ones.\n",
    "\n",
    "We are going to create a multistep job that gets the most frequent word in the text and only returns that one.\n",
    "It's clear that the first loop through MapReduce will be our previous word frequency counter, then we need to filter the most frequent word out of all the counted ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRMostFreqWord(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_wordcount,\n",
    "                    reducer=self.reducer_wordcount),\n",
    "            MRStep(mapper=self.mapper_freq,\n",
    "                    reducer=self.reducer_freq),\n",
    "            MRStep(mapper=self.mapper_most,\n",
    "                    reducer=self.reducer_most)\n",
    "        ]\n",
    "\n",
    "    def mapper_wordcount(self, _, line):\n",
    "        for word in line.split():\n",
    "            if len(word)>2:\n",
    "                yield word.lower(), 1 # return only word with at least 3 letters\n",
    "\n",
    "    def reducer_wordcount(self, word, counts):\n",
    "        yield word, sum(counts)  # Sum occurrences of each word to get frequency\n",
    "\n",
    "    def mapper_freq(self, word, total):\n",
    "        if total > 1:  # Only get words that appear more than once\n",
    "            yield total, word  # Group them by frequency\n",
    "\n",
    "    def reducer_freq(self, total, words):\n",
    "        yield total, words.next()  # .next() gets the first element, so we emit only one word for each frequency\n",
    "\n",
    "    def mapper_most(self, freq, word):\n",
    "        yield 'most_used', [freq, word]  # Group all the words together in a list of (frequency, word) tuples\n",
    "\n",
    "    def reducer_most(self, _, freqs):\n",
    "        yield 'most_used', max(freqs)  # Get only the most frequent word\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        MRMostFreqWord.run()\n",
    "    except TypeError:\n",
    "        print 'MrJob cannot work inside iPython Notebook as it is not saved as a standalone .py file'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of our script when launched against the lorem.txt file will be:\n",
    "\n",
    "```\n",
    "\"most_used\"\t[4, \"nec\"]\n",
    "```\n",
    "\n",
    "Which is actually a conjuction, so our rule of filtering words shorter than 3 characters didn't remove all the conjunctions, we can try to run again the script using a better filter (using for example regular expressions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "## MultiStep Example\n",
    "WORD_REGEXP = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRMostFreqWord(MRJob):\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_wordcount,\n",
    "                    reducer=self.reducer_wordcount),\n",
    "            MRStep(mapper=self.mapper_freq,\n",
    "                    reducer=self.reducer_freq),\n",
    "            MRStep(mapper=self.mapper_most,\n",
    "                    reducer=self.reducer_most)\n",
    "        ]\n",
    "\n",
    "    def mapper_wordcount(self, _, line):\n",
    "        words = WORD_REGEXP.findall(line)\n",
    "        for w in words:\n",
    "            if len(w)>3:\n",
    "                yield w.lower(), 1\n",
    "        #for word in line.split():\n",
    "        #    if len(word)>4:\n",
    "        #        yield word.lower(), 1 # return only word with at least 3 letters\n",
    "\n",
    "    def reducer_wordcount(self, word, counts):\n",
    "        yield word, sum(counts)  # Sum occurrences of each word to get frequency\n",
    "\n",
    "    def mapper_freq(self, word, total):\n",
    "        if total > 1:  # Only get words that appear more than once\n",
    "            yield total, word  # Group them by frequency\n",
    "\n",
    "    def reducer_freq(self, total, words):\n",
    "        yield total, words.next()  # .next() gets the first element, so we emit only one word for each frequency\n",
    "\n",
    "    def mapper_most(self, freq, word):\n",
    "        yield 'most_used', [freq, word]  # Group all the words together in a list of (frequency, word) tuples\n",
    "\n",
    "    def reducer_most(self, _, freqs):\n",
    "        yield 'most_used', max(freqs)  # Get only the most frequent word\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        MRMostFreqWord.run()\n",
    "    except TypeError:\n",
    "        print 'MrJob cannot work inside iPython Notebook as it is not saved as a standalone .py file'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we will have a better result:\n",
    "\n",
    "```\n",
    "\"most_used\"\t[4, \"vitae\"]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Play! \n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "From the lorem.txt file calculate how many words starts for each letter of the alphabet.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "From the lorem.txt file calculate how many words starts for each letter of the alphabet and print out the max and the min.\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "Write a MapReduce job that report the most frequent word grouped by word length.\n",
    "\n",
    "ES:\n",
    "\n",
    "```\n",
    "5 Chars -> hello -> 8 occurrences\n",
    "3 Chars -> cat -> 4 occurrences\n",
    "2 Chars -> at -> 7 occurrences\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmatically Running Jobs\n",
    "\n",
    "While starting jobs from the command line is a good way to test them, it is often the case that you have to visualize the resulting data. So we need to be able to start the MapReduce job from our software, get the output and send it to the HTML layer for visualization.\n",
    "\n",
    "This can be achieved using MrJob Runners, which permit to start the execution programmatically and read the output.\n",
    "Keep in mind that, as the MrJob process must live in a separate **.py** file, the runner must be kept separate from it.\n",
    "So the runner will rely in our application, while the actual MrJob class will be in a separate module that can be sent to Hadoop for execution.\n",
    "\n",
    "Our runner for the WordFreqCount job will look like (check the folder 06_runner):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcount import MRWordFreqCount\n",
    "\n",
    "mr_job = MRWordFreqCount()\n",
    "mr_job.stdin = open('lorem.txt').readlines()\n",
    "\n",
    "with mr_job.make_runner() as runner:\n",
    "    runner.run()\n",
    "    for line in runner.stream_output():\n",
    "        key, value = mr_job.parse_output_line(line)\n",
    "        print 'Word:', key, 'Count:', value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``stdin`` parameter of the job is the actual input it will receive.\n",
    "As Hadoop inputs are line separated we need to pass a list of strings, one for each line. \n",
    "Being our data already text on multiple lines we can just read the lines in the text file.\n",
    "\n",
    "Then the runner will provide back the output through the ``stream_output`` function, which is a generator that returns a single HadoopStreaming output line. This has to be parsed according to the communication protocol, so we need to call ``parse_output_line`` to get back the emitted **key** and **value**.\n",
    "\n",
    "Then we have the emitted values and we can use them as needed. In this case we just print them.\n",
    "Note that differently from when you run MrJob manually, in this case there is no output apart from our own prints.\n",
    "\n",
    "So if you start the runner without printing anything it will just do nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - MovieData DB\n",
    "\n",
    "* Download MovieData DB (100K) from http://grouplens.org/datasets/movielens/\n",
    "\n",
    "I already downloaded the dataset inside the folder ``/PATH/TO/MRJOB/ROOT/examples/_dataset``. Unzip the folder and let's start!\n",
    "\n",
    "We have (from ``u.info`` file):\n",
    "\n",
    "    943 users\n",
    "    1682 films\n",
    "    100000 ratings\n",
    "    \n",
    "We'll use the file u.data which contains (splitted by TAB):\n",
    "\n",
    "    user id | film id | rating | timestamp\n",
    "       299     144        4      877881320\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 4 - Rating Counter\n",
    "\n",
    "Count occurrences of rating value from movie DB.\n",
    "\n",
    "### Exercise 5 - Most Rated Movie\n",
    "\n",
    "Count occurrences of each movie rating from movie DB and find the most rated.\n",
    "\n",
    "### Exercise 6 - Quick Lookup\n",
    "\n",
    "Add to the exercise 2 the information about the movie. (use file ``u.item``)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Fake Friends DB\n",
    "\n",
    "Inside folder ``/PATH/TO/MRJOB/ROOT/examples/_dataset`` you will find a csv file called ``fakefriends.csv``. Inside this file there is fake list of users with the relative friends. This is the format:\n",
    "\n",
    "    ID, Name, Age, Number of Friends\n",
    "    0,Will,33,385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 - User with max friends\n",
    "\n",
    "Find the user which has the MAX number of friends and to the same for the MIN.\n",
    "\n",
    "### Exercise 8 - Friends Avarage per Age\n",
    "\n",
    "Calculate For each Age the Avarage of friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
